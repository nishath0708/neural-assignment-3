{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95f474e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['NLP', 'techniques', 'are', 'used', 'in', 'virtual', 'assistants', 'like', 'Alexa', 'and', 'Siri.']\n",
      "After Stopword Removal: ['NLP', 'techniques', 'used', 'virtual', 'assistants', 'Alexa', 'Siri.']\n",
      "After Stemming: ['NLP', 'technique', 'used', 'virtual', 'assistant', 'Alexa', 'Siri.']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def preprocess_nlp_basic(sentence):\n",
    "    # Step 1: Tokenize by splitting on spaces\n",
    "    tokens = sentence.split()\n",
    "    print(\"Tokens:\", tokens)\n",
    "    \n",
    "    # Step 2: Remove common English stopwords (basic list)\n",
    "    stop_words = {\n",
    "        \"a\", \"an\", \"the\", \"and\", \"or\", \"in\", \"on\", \"at\", \"to\", \"for\", \"of\", \"are\", \"is\", \"was\", \"were\", \"be\", \"been\",\n",
    "        \"has\", \"have\", \"had\", \"do\", \"does\", \"did\", \"with\", \"like\", \"as\", \"by\", \"from\", \"that\", \"this\", \"it\", \"you\", \"i\"\n",
    "    }\n",
    "    tokens_no_stopwords = [word for word in tokens if word.lower() not in stop_words]\n",
    "    print(\"After Stopword Removal:\", tokens_no_stopwords)\n",
    "    \n",
    "    # Step 3: Simple stemming - just remove common suffixes (very basic)\n",
    "    suffixes = ['ing', 'ly', 'ed', 's', 'es']\n",
    "    def simple_stem(word):\n",
    "        for suffix in suffixes:\n",
    "            if word.lower().endswith(suffix) and len(word) > len(suffix) + 2:\n",
    "                return word[:-len(suffix)]\n",
    "        return word\n",
    "    \n",
    "    stemmed_tokens = [simple_stem(word) for word in tokens_no_stopwords]\n",
    "    print(\"After Stemming:\", stemmed_tokens)\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"NLP techniques are used in virtual assistants like Alexa and Siri.\"\n",
    "preprocess_nlp_basic(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f43e9ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities Found:\n",
      "\n",
      "Entity: Barack, Label: PERSON\n",
      "Entity: Obama, Label: PERSON\n",
      "Entity: United States, Label: GPE\n",
      "Entity: Nobel Peace Prize, Label: ORGANIZATION\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Path to local NLTK data folder\n",
    "nltk_data_path = os.path.join(str(pathlib.Path.home()), \"nltk_data\")\n",
    "\n",
    "# Check and download only if missing (no output shown)\n",
    "def silent_nltk_download(package):\n",
    "    try:\n",
    "        nltk.data.find(package)\n",
    "    except LookupError:\n",
    "        nltk.download(package.split(\"/\")[-1], quiet=True)\n",
    "\n",
    "# Download only if not present\n",
    "silent_nltk_download('tokenizers/punkt')\n",
    "silent_nltk_download('taggers/averaged_perceptron_tagger')\n",
    "silent_nltk_download('chunkers/maxent_ne_chunker')\n",
    "silent_nltk_download('corpora/words')\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009.\"\n",
    "\n",
    "# Tokenize, POS tag, and perform Named Entity Recognition\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "\n",
    "# Output named entities\n",
    "print(\"Named Entities Found:\\n\")\n",
    "for chunk in named_entities:\n",
    "    if hasattr(chunk, 'label'):\n",
    "        entity = \" \".join(c[0] for c in chunk)\n",
    "        label = chunk.label()\n",
    "        print(f\"Entity: {entity}, Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bd9b666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['NLP', 'techniques', 'are', 'used', 'in', 'virtual', 'assistants', 'like', 'Alexa', 'and', 'Siri']\n",
      "After Stopword Removal: ['NLP', 'techniques', 'used', 'virtual', 'assistants', 'Alexa', 'Siri']\n",
      "After Stemming: ['NLP', 'technique', 'used', 'virtual', 'assistant', 'Alexa', 'Siri']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    # Replace punctuation with spaces, then split\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    clean_text = text.translate(translator)\n",
    "    tokens = clean_text.split()\n",
    "    return tokens\n",
    "\n",
    "def simple_stopword_removal(tokens):\n",
    "    stopwords = {\n",
    "        \"a\", \"an\", \"the\", \"and\", \"or\", \"in\", \"on\", \"at\", \"to\", \"for\", \"of\",\n",
    "        \"are\", \"is\", \"was\", \"were\", \"be\", \"been\", \"has\", \"have\", \"had\",\n",
    "        \"do\", \"does\", \"did\", \"with\", \"like\", \"as\", \"by\", \"from\", \"that\",\n",
    "        \"this\", \"it\", \"you\", \"i\"\n",
    "    }\n",
    "    return [t for t in tokens if t.lower() not in stopwords]\n",
    "\n",
    "def simple_stem(tokens):\n",
    "    suffixes = ['ing', 'ly', 'ed', 's', 'es']\n",
    "    stemmed = []\n",
    "    for word in tokens:\n",
    "        for suffix in suffixes:\n",
    "            if word.lower().endswith(suffix) and len(word) > len(suffix) + 2:\n",
    "                word = word[:-len(suffix)]\n",
    "                break\n",
    "        stemmed.append(word)\n",
    "    return stemmed\n",
    "\n",
    "def preprocess_nlp(text):\n",
    "    tokens = simple_tokenize(text)\n",
    "    print(\"Tokens:\", tokens)\n",
    "    tokens_no_stopwords = simple_stopword_removal(tokens)\n",
    "    print(\"After Stopword Removal:\", tokens_no_stopwords)\n",
    "    stemmed_tokens = simple_stem(tokens_no_stopwords)\n",
    "    print(\"After Stemming:\", stemmed_tokens)\n",
    "\n",
    "# Example\n",
    "sentence = \"NLP techniques are used in virtual assistants like Alexa and Siri.\"\n",
    "preprocess_nlp(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f39b9ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: NEGATIVE\n",
      "Confidence Score: 0.0833\n"
     ]
    }
   ],
   "source": [
    "# Simple rule-based sentiment analysis without NLTK\n",
    "\n",
    "positive_words = {\"good\", \"great\", \"excellent\", \"outstanding\", \"happy\", \"love\", \"awesome\", \"best\", \"fantastic\", \"amazing\"}\n",
    "negative_words = {\"bad\", \"terrible\", \"poor\", \"hate\", \"worst\", \"awful\", \"disappointing\", \"high\", \"expensive\"}\n",
    "\n",
    "def simple_sentiment_analysis(text):\n",
    "    words = text.lower().split()\n",
    "    pos_count = sum(word in positive_words for word in words)\n",
    "    neg_count = sum(word in negative_words for word in words)\n",
    "\n",
    "    if pos_count > neg_count:\n",
    "        sentiment = \"POSITIVE\"\n",
    "        confidence = pos_count / len(words)\n",
    "    elif neg_count > pos_count:\n",
    "        sentiment = \"NEGATIVE\"\n",
    "        confidence = neg_count / len(words)\n",
    "    else:\n",
    "        sentiment = \"NEUTRAL\"\n",
    "        confidence = 0.0\n",
    "\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print(f\"Confidence Score: {confidence:.4f}\")\n",
    "\n",
    "# Example\n",
    "sentence = \"Despite the high price, the performance of the new MacBook is outstanding.\"\n",
    "simple_sentiment_analysis(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7b2114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
